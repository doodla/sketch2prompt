spec_version: "1.0"
component_id: "node_sicFMoQ5zO"
name: "Vector Store"
type: "storage"
description: |
  Durable storage layer optimized for high-dimensional embedding vectors and fast similarity search. Supports upsert, delete, and retrieval of vectors with associated metadata, organized by collection/namespace. Provides scalable indexing strategies for efficient k-NN queries used by upstream application logic.
responsibilities:
  - Persist embedding vectors with metadata in a durable, queryable store
  - Provide similarity search (k-NN) and filtered retrieval across namespaces/collections
  - Support bulk upsert, idempotent writes, and safe deletes with audit timestamps
  - Expose configurable distance metrics and index parameters aligned with upstream embeddings
  - Enforce multi-tenant isolation via namespaces and optional per-tenant collections
anti_responsibilities:
  - NEVER compute embeddings — upstream Embeddings component is the sole producer of vectors
  - NEVER implement business ranking logic — only return similarity scores and stored metadata
  - NEVER expose sensitive metadata or raw vectors in logs — protect privacy and compliance
  - NEVER bypass authorization/namespace checks — preserves tenant isolation and data integrity
  - NEVER return raw database errors to callers — map to safe, structured error responses

integration_points:
  - component: Embeddings
    direction: inbound
    purpose: Receive newly generated vectors and metadata for upsert into collections/namespaces.
    contract:
      request: |
        {
          "collection": "string",
          "namespace": "string",            // optional; defaults to "default"
          "dimension": 1536,                // required; must match vector length
          "items": [
            {
              "id": "string",
              "vector": [number, ...],      // length = dimension
              "metadata": { "any": "json" },// optional JSON-serializable metadata
              "docRef": "string"            // optional external reference
            }
          ],
          "upsertOptions": {
            "idempotencyKey": "string",     // optional for deduplication
            "updateIfExists": true
          }
        }
      response: |
        {
          "status": "ok" | "partial" | "error",
          "upserted": ["id", "..."],
          "failed": [
            { "id": "string", "errorCode": "string", "message": "string" }
          ]
        }

tech_stack:
  primary: "PostgreSQL with pgvector extension (cosine or inner product distance) for on-prem/self-hosted vector storage"
  baseline_deps:
    - pgvector
    - PostgreSQL
    - database client for application runtime (e.g., Node pg, Python psycopg)
  references: []

validation:
  exit_criteria:
    - Tables, extensions, and indexes created for configured collections/namespaces
    - Upsert and k-NN query APIs operational with expected latency under target SLOs
    - Dimension and metric validation enforced at write-time
    - Namespaced data isolation verified
    - Backup/restore tested successfully for at least one collection
    - Status file updated with component completion
  smoke_tests:
    - Verify pgvector extension installed and enabled (CREATE EXTENSION IF NOT EXISTS vector)
    - Create test collection and insert 3 vectors with known neighbors
    - Run a k-NN query with topK=2 using cosine distance and confirm expected ordering
    - Upsert the same id twice with idempotencyKey and confirm single row version
    - Delete an id and confirm it is not returned by subsequent searches
  integration_checks:
    - Embeddings inbound: Reject payloads where vector length != declared dimension
    - Embeddings inbound: Accept batch upsert and return all upserted ids
    - Embeddings inbound: Persist metadata and verify it is filterable (e.g., JSONB filters)
    - Embeddings inbound: Enforce namespace and collection existence or auto-create if configured
    - Embeddings inbound: Return structured errors for invalid metric/dimension mismatch

schema_notes:
  - Collections table: collections(id PK, name unique, dimension int, metric text, created_at, updated_at)
  - Namespaces table (optional): namespaces(id PK, name unique, created_at)
  - Items table: items(id PK, collection_id FK, namespace_id FK nullable, vector vector(dim), metadata jsonb, doc_ref text, created_at, updated_at, deleted_at nullable)
  - Relationships: items.collection_id -> collections.id; items.namespace_id -> namespaces.id
  - Constraints:
    - Check: vector length equals collections.dimension
    - Check: metric in ('cosine','ip','l2')
    - Partial index to exclude soft-deleted rows (deleted_at IS NULL)
backup_strategy:
  - Daily logical dumps (pg_dump) of collections, namespaces, and items with WAL archiving enabled for PITR
  - Store encrypted backups in off-site object storage with minimum 14-day retention
  - Quarterly restore drills to a staging environment validating index recreation and query correctness
  - For large datasets, consider partition-level backups and parallel dumps to reduce RTO
indexing_strategy:
  - Use pgvector indexes per collection:
    - ivfflat or hnsw depending on Postgres/pgvector version and workload
    - Default metric: cosine for normalized embeddings; inner product if upstream model expects IP
  - Recommended parameters:
    - ivfflat: lists = sqrt(rowcount) rounded, probe = 10–20; tune per latency/recall targets
    - hnsw: m = 16–32, ef_construction = 200–400, ef_search = 40–100
  - Secondary indexes:
    - GIN index on metadata for common filter keys (jsonb_path_ops)
    - B-tree indexes on (collection_id, namespace_id) and created_at for scans/TTL tasks
  - Partition items by collection and optionally by namespace for large-scale datasets
  - Maintain ANALYZE/VACUUM policies and periodic reindex where appropriate to sustain performance